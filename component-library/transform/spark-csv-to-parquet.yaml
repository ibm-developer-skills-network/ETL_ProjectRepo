name: spark-csv-to-parquet
description: Converts a CSV file with header to parquet using ApacheSpark

inputs:
- {name: data_csv, type: String, description: 'source path and file name (default: data.csv)'}
- {name: master, type: String, description: 'url of master (default: local mode)'}
- {name: data_dir, type: String, description: 'temporal data storage for local execution'}


outputs:
- {name: output_data_parquet, type: String, description: 'destination path and parquet file name (default: data.parquet)'}


implementation:
    container:
        image: continuumio/anaconda3:2020.07
        command:
        - sh
        - -ec
        - |
          mkdir -p `echo $0 |sed -e 's/\/[a-zA-Z0-9]*$//'`
          wget https://raw.githubusercontent.com/IBM/claimed/master/component-library/input/input-postgresql.ipynb
          ipython ./input-postgresql.ipynb output_data_csv="$0" host="$1" database="$2" user="$3" password="$4" port="$5" sql="$6" data_dir="$7"
        - {outputPath: output_data_parquet}
        - {inputValue: data_csv}
        - {inputValue: master}
        - {inputValue: data_dir}
