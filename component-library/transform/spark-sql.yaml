name: SparkSQL
description: Execute arbitrary SQL queries againts CSV and PARQUET files

inputs:
- {name: data_file, type: String, description: 'file name for CSV or PARQUET file - must end with .csv or .parquet (default: data.csv)'}
- {name: master, type: String, description: 'master url of spark master (default: local mode)'}
- {name: data_dir, type: String, description: 'sql statement to execute, table name == df, example: select * from df'}


outputs:
- {name: output_result_file, type: String, description: 'name of resulting file (default: data_result.csv)'}


implementation:
    container:
        image: continuumio/anaconda3:2020.07
        command:
        - sh
        - -ec
        - |
          mkdir -p `echo $0 |sed -e 's/\/[a-zA-Z0-9]*$//'`
          wget https://raw.githubusercontent.com/IBM/claimed/master/component-library/input/input-postgresql.ipynb
          ipython ./input-postgresql.ipynb output_data_csv="$0" host="$1" database="$2" user="$3" password="$4" port="$5" sql="$6" data_dir="$7"
        - {outputPath: output_result_file}
        - {inputValue: data_file}
        - {inputValue: master}
        - {inputValue: data_dir}
